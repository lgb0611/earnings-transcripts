from googlesearch import search
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin, urlparse
import re
from typing import List, Optional, Dict
import time
from config import GOOGLE_SEARCH_QUERY_TEMPLATE

class EarningsCallSearcher:
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        ]
    
    def search_earnings_transcript(self, ticker: str, search_query: str) -> List[Dict]:
        """í‹°ì»¤ë¡œ ìµœê·¼ ì–´ë‹ ì»¨ì½œ ìŠ¤í¬ë¦½íŠ¸ ê²€ìƒ‰"""
        results = []
        
        try:
            print(f"ðŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
            for url in search(search_query, num_results=10, lang='en'):
                if self._is_valid_transcript_url(url):
                    content = self._extract_transcript(url)
                    if content and len(content) > 500:
                        results.append({
                            'url': url,
                            'title': self._get_title(url),
                            'content': content,
                            'ticker': ticker
                        })
                        if len(results) >= 3:
                            break
                time.sleep(1)
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return results
    
    def _is_valid_transcript_url(self, url: str) -> bool:
        """ì–´ë‹ ì»¨ì½œ ìŠ¤í¬ë¦½íŠ¸ URLì¸ì§€ ê²€ì¦"""
        valid_domains = [
            'seekingalpha.com', 'fool.com', 'yahoo.com', 
            'investing.com', 'motleyfool.com', 'nasdaq.com',
            'marketbeat.com', 'gurufocus.com'
        ]
        return any(domain in url.lower() for domain in valid_domains)
    
    def _extract_transcript(self, url: str) -> Optional[str]:
        """ì›¹íŽ˜ì´ì§€ì—ì„œ íŠ¸ëžœìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ"""
        try:
            headers = {'User-Agent': self.user_agents[0]}
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë‹¤ì–‘í•œ ì„ íƒìžë¡œ íŠ¸ëžœìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
            selectors = [
                '.transcript', '.call-transcript', '[class*="transcript"]',
                '.call-text', '[class*="call"]', '.post-body',
                'article', '.content', 'main', '.entry-content'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    text = ' '.join([el.get_text() for el in elements])
                    cleaned_text = self._clean_text(text)
                    if len(cleaned_text) > 1000:
                        return cleaned_text
            
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            text = soup.get_text()
            return self._clean_text(text)
            
        except Exception as e:
            print(f"íŽ˜ì´ì§€ ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {e}")
            return None
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\'\"\n]', '', text)
        sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 20]
        return '. '.join(sentences[:100])
    
    def _get_title(self, url: str) -> str:
        """ì œëª© ì¶”ì¶œ"""
        domain = urlparse(url).netloc
        return f"{domain} - Earnings Call Transcript"